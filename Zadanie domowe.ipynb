{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inżynieria lingwistyczna\n",
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1 - tokenizacja (12 pkt)\n",
    "\n",
    "Jedną z nowoczesnych technik tokenizacji jest BPE - byte-pair encoding [1]. Technika ta polega na podzielenie słów na częste podsłowa (morfemy). W przeciwieństwie do podejść lingwistycznych, wymagających reguł tworzenia morfemów, BPE wyznacza je automatycznie poprzez wyznaczenie najczęstszych przylegających do siebie sekwencji znaków które występują obok siebie.\n",
    "\n",
    "Algorytm przebiega w następujących krokach.\n",
    "1. Podziel wszystkie słowa na symbole (początkowo pojedyncze znaki)\n",
    "2. Wyznacz najczęściej występującą obok siebie parę symboli \n",
    "3. Stwórz nowy symbol będący konkatenacją dwóch najczęstszych symboli.\n",
    "\n",
    "Uwaga 1: każde słowo zakończone jest specjalnym symbolem końca wyrazu.\n",
    "\n",
    "Uwaga 2: tworzenie nowego symbolu nie powoduje usuniecie starego tj. zawsze jednym z możliwych symboli jest pojedynczy znak, ale jeśli można to stosujemy symbol dłuższy.\n",
    "\n",
    "Przykład: korpus w którym występuje ,,ala'' 5 razy i ,,mama 10 razy''\n",
    "1. Dzielimy słowa na symbole ,,a l a END'' ,,m a m a END''  gdzie END jest symbolem końca wyrazu.\n",
    "2. Najczęstsza para obok siebie to ,,m a'' (20) razy\n",
    "3. Nowy symbol ,,ma''\n",
    "4. Nowy podział ,,a l a END'' ,,ma ma END''\n",
    "5. Najczęstsza para ,,ma ma'' (10) razy\n",
    "6. Nowy symbol ,,mama''\n",
    "7. Nowy podział ,,a l a END'' ,,mama END''\n",
    "8. itd.\n",
    "\n",
    "W pliku ,,brown_clusters.tsv'' pierwsza kolumna to identyfikator skupienia (nie używamy w tym zadaniu), druga kolumna to wyrazy, a trzecia to ich liczności w pewnym korpusie tweetów. Zaimplementuj technike BPE na tych słowach.\n",
    "\n",
    "Zaimplementuj algorytm BPE wykonujący `number_of_iterations` iteracji (łączeń symboli).\n",
    "\n",
    "[1] Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In ACL 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ff3b90528fdb50de90c5c946c157e21",
     "grade": false,
     "grade_id": "cell-93d78a28d4e25cbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "brown_df = pd.read_csv('brown_clusters.tsv', sep='\\t', header=0, names=['cluster', 'word', 'count'])\n",
    "\n",
    "END_SYMBOL = 'END'\n",
    "number_of_iterations = 10\n",
    "\n",
    "\n",
    "def concatenate_words(splitted_word, first_elem, second_elem, concatenated):\n",
    "    # Preapre output\n",
    "    result = list()\n",
    "    \n",
    "    # Last seen element\n",
    "    last_seen_elem = None\n",
    "    \n",
    "    for elem in splitted_word:\n",
    "        if last_seen_elem is not None:\n",
    "            # We must check both elements\n",
    "            if last_seen_elem == first_elem and elem == second_elem:\n",
    "                # Add concatenated word to result\n",
    "                result.append(concatenated)\n",
    "                \n",
    "                # Clear last element to prevent duplications in replacement\n",
    "                last_seen_elem = None\n",
    "                \n",
    "                # Check next element\n",
    "                continue\n",
    "            else:\n",
    "                # Store previous element\n",
    "                result.append(last_seen_elem)\n",
    "            \n",
    "        # Update last seen element\n",
    "        last_seen_elem = elem\n",
    "        \n",
    "    # Add last element if not paired\n",
    "    if last_seen_elem is not None:\n",
    "        result.append(last_seen_elem)\n",
    "        \n",
    "    # Return joined list\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_end_tag(splitted_word):\n",
    "    splitted_word.append(END_SYMBOL)\n",
    "    return splitted_word\n",
    "\n",
    "\n",
    "def output(df):\n",
    "    \"\"\"\n",
    "    Generate output as list of words, each with <END> tag\n",
    "    \"\"\"\n",
    "    return df['word'].apply(lambda word: ' '.join(word)).tolist()\n",
    "    \n",
    "\n",
    "def preform_bpe(brown_df, number_of_iterations):\n",
    "    \"\"\"\n",
    "    Funkcja przyjmuje ramkę w formacie analogicznym do obiektu brown_df (wczytany wyżej)\n",
    "     oraz liczbę iteracji.\n",
    "    Wyjściem funkcji powinna być lista słów z poszczególnymi tokenami/symbolami oddzielonymi spacją.\n",
    "    Za znak końca wyrazu przyjmij END. \n",
    "    \"\"\"\n",
    "    # Copy dataframe\n",
    "    df = brown_df.copy()\n",
    "    \n",
    "    # Ensure word is string\n",
    "    df['word'] = df['word'].astype(str)\n",
    "    \n",
    "    # Split each character\n",
    "    df['word'] = df['word'].apply(lambda word: list(word))\n",
    "    \n",
    "    # Add <END> tag\n",
    "    df['word'] = df['word'].apply(add_end_tag)\n",
    "    \n",
    "    # Do it n-times\n",
    "    for _iteration in range(number_of_iterations):\n",
    "        # Vocabulary counter - we will use it to find the best pair\n",
    "        vocabulary = Counter()\n",
    "\n",
    "        # Iterate over elements in corpus\n",
    "        for row in df.itertuples():\n",
    "            last_element = None\n",
    "            for element in row.word:\n",
    "                if last_element is not None:\n",
    "                    # We already seen element - can generate new pair\n",
    "                    vocabulary[(last_element, element)] += row.count\n",
    "\n",
    "                # Update last seen element\n",
    "                last_element = element\n",
    "            \n",
    "        # Now we must ensure at least one pair found\n",
    "        if not vocabulary:\n",
    "            return output(df)\n",
    "\n",
    "        # We can find maximum element\n",
    "        ((first_elem, second_elem), cardinality) = vocabulary.most_common(1)[0]\n",
    "        concatenated = first_elem + second_elem\n",
    "        print(first_elem + ' AND ' + second_elem + ' ====> ' + concatenated)\n",
    "    \n",
    "        # Update words - join group \n",
    "        df['word'] = df['word'].apply(concatenate_words, args = (first_elem, second_elem, concatenated))\n",
    "    \n",
    "    # Return dataframe\n",
    "    return output(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfff70f711bf389f0f1cd969e7c3a413",
     "grade": true,
     "grade_id": "cell-7e952fa8dcd136fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m AND a ====> ma\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_list_equal\n",
    "data = {'cluster': range(2), 'word':['ala', 'mama'], 'count': [5,10]}\n",
    "df = pd.DataFrame (data, columns = ['cluster', 'word', 'count'])\n",
    "vocab = preform_bpe(df, 1)\n",
    "assert_list_equal(vocab, ['a l a END', 'ma ma END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spraw aby Twoja implementacja wypisywała kolejne łączone ze sobą symbole i uruchom Twoją funkcję na np. 50 iteracji, obserwując jakie tokeny są tworzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e AND END ====> eEND\n",
      "t AND END ====> tEND\n",
      "s AND END ====> sEND\n",
      "i AND n ====> in\n",
      "t AND h ====> th\n",
      "d AND END ====> dEND\n",
      "y AND END ====> yEND\n",
      ". AND END ====> .END\n",
      "o AND END ====> oEND\n",
      "r AND END ====> rEND\n",
      "a AND n ====> an\n",
      "> AND END ====> >END\n",
      "o AND n ====> on\n",
      "o AND u ====> ou\n",
      "g AND END ====> gEND\n",
      "a AND END ====> aEND\n",
      "l AND END ====> lEND\n",
      "in AND gEND ====> ingEND\n",
      "< AND @ ====> <@\n",
      "<@ AND M ====> <@M\n",
      "<@M AND E ====> <@ME\n",
      "<@ME AND N ====> <@MEN\n",
      "<@MEN AND T ====> <@MENT\n",
      "<@MENT AND I ====> <@MENTI\n",
      "<@MENTI AND O ====> <@MENTIO\n",
      "<@MENTIO AND N ====> <@MENTION\n",
      "<@MENTION AND >END ====> <@MENTION>END\n",
      "r AND e ====> re\n",
      "i AND END ====> iEND\n",
      "th AND eEND ====> theEND\n",
      "e AND n ====> en\n",
      "o AND m ====> om\n",
      "t AND oEND ====> toEND\n",
      ", AND END ====> ,END\n",
      "! AND END ====> !END\n",
      "e AND r ====> er\n",
      "h AND a ====> ha\n",
      "e AND rEND ====> erEND\n",
      "i AND t ====> it\n",
      ": AND END ====> :END\n",
      "y AND ou ====> you\n",
      "a AND r ====> ar\n",
      "a AND l ====> al\n",
      "o AND r ====> or\n",
      "o AND w ====> ow\n",
      ". AND .END ====> ..END\n",
      "s AND t ====> st\n",
      "k AND END ====> kEND\n",
      "i AND sEND ====> isEND\n",
      "f AND END ====> fEND\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\ iEND',\n",
       " '/ i / END',\n",
       " 't o d a y - iEND',\n",
       " 'n ow iEND',\n",
       " '# you e v erEND',\n",
       " 'i f in al l yEND',\n",
       " '「 iEND',\n",
       " '- i - END',\n",
       " 'in e v aEND',\n",
       " '» iEND',\n",
       " 'w ha t t a y aEND',\n",
       " 'i i i i i i i i i iEND',\n",
       " '\\ue6d1 END',\n",
       " 'i k in d aEND',\n",
       " 'l o l - iEND',\n",
       " 'i a c t u al l yEND',\n",
       " 'w a d d y aEND',\n",
       " '# a s l on g a s you END',\n",
       " 'd o you END',\n",
       " '\\u200e \\u200b iEND',\n",
       " 'i ̇ END',\n",
       " 'ï END',\n",
       " '# l o l a t g i r l s w h oEND',\n",
       " '# r t i f you END',\n",
       " 'i j s tEND',\n",
       " '« iEND',\n",
       " '• iEND',\n",
       " 'w h o d aEND',\n",
       " 'w ha d y aEND',\n",
       " ') iEND',\n",
       " '+ iEND',\n",
       " '# you r f a c e m a k e s m eEND',\n",
       " 'i i i i i i i iEND',\n",
       " '` iEND',\n",
       " 'i i i i i i iEND',\n",
       " 'i al re a d yEND',\n",
       " '_ iEND',\n",
       " '# you m a k e m eEND',\n",
       " '* iEND',\n",
       " '| iEND',\n",
       " '# u r b o y f r i en d e v erEND',\n",
       " 'w h en iEND',\n",
       " 'ι END',\n",
       " \"d on ' t c h aEND\",\n",
       " \"w h o ' d aEND\",\n",
       " 'd you END',\n",
       " 'w ha d d a y aEND',\n",
       " 'i on l yEND',\n",
       " 'i j u s sEND',\n",
       " 'i al w a y sEND',\n",
       " 'i i i i iEND',\n",
       " 'd on c h aEND',\n",
       " '( iEND',\n",
       " \"d ' y aEND\",\n",
       " 'ı END',\n",
       " '# u e v erEND',\n",
       " 'in e v erEND',\n",
       " 'i - iEND',\n",
       " 'i j u sEND',\n",
       " '/ / iEND',\n",
       " 'i st i l lEND',\n",
       " 'w ha d d y aEND',\n",
       " \"d ' you END\",\n",
       " 'i re al l yEND',\n",
       " 'd on t c h aEND',\n",
       " 'i j u s tEND',\n",
       " 'iEND',\n",
       " '- iEND',\n",
       " 'i you END',\n",
       " '# in n ow a y s ha p e or f or m END',\n",
       " '( you END',\n",
       " '/ / w eEND',\n",
       " '/ / u END',\n",
       " '# m en m ar r y w om en th a tEND',\n",
       " '/ w eEND',\n",
       " 's e l f - e d u c a t i on END',\n",
       " '# re al g r an d m a sEND',\n",
       " '/ you END',\n",
       " '# s h ou t ou t t o g i r l s w h oEND',\n",
       " '# b o y s w h oEND',\n",
       " 'i / w eEND',\n",
       " '# s h ou t ou t t o th e g u y s th a tEND',\n",
       " '/ / you END',\n",
       " '# i l o v e p e o p l e th a tEND',\n",
       " '# n o t al l b l a c k p e o p l eEND',\n",
       " '# i c an t st an d p e o p l e th a tEND',\n",
       " '# s h ou t ou t t o th e g i r l s th a tEND',\n",
       " '- th e yEND',\n",
       " '- w eEND',\n",
       " '# h ow m an y p e o p l eEND',\n",
       " '- you END',\n",
       " 'w eEND',\n",
       " '# a q u ar i an sEND',\n",
       " 't th e yEND',\n",
       " 'th w yEND',\n",
       " 'g u i l d en st er n END',\n",
       " \"d ' u END\",\n",
       " '# i ha t e m al e s w h oEND',\n",
       " 't e h yEND',\n",
       " 'th r yEND',\n",
       " 'i f you END',\n",
       " '# h ou s e h i p p o sEND',\n",
       " 'th e u END',\n",
       " 'th e e yEND',\n",
       " '# i ha t e f e m al e s w h oEND',\n",
       " 'th e y yEND',\n",
       " 'th e yEND',\n",
       " 'v i o l e t sEND',\n",
       " 'e h oEND',\n",
       " 'w h o ’ dEND',\n",
       " 'w h o t fEND',\n",
       " 'w h o ’ v eEND',\n",
       " 'w h o dEND',\n",
       " '< U R L - re al . c om >END',\n",
       " '# i l i k e p e o p l e w h oEND',\n",
       " '- w h oEND',\n",
       " 'w h 0 END',\n",
       " 'w h u END',\n",
       " 'w h oEND',\n",
       " \"w h o ' v eEND\",\n",
       " 's s h eEND',\n",
       " 's er - u e b er w a c h erEND',\n",
       " 's h h eEND',\n",
       " 't e st a st er i s kEND',\n",
       " '# m y d u m b a s sEND',\n",
       " 's j eEND',\n",
       " 't a c h om a st erEND',\n",
       " 'i al m o s tEND',\n",
       " 'i d on eEND',\n",
       " '# w ha t i f iEND',\n",
       " 'h e / s h e / i tEND',\n",
       " '$ h eEND',\n",
       " '# w ha t i f g o dEND',\n",
       " '# i h e ar d c h u c k n or r isEND',\n",
       " '# f m h 2 0 1 1 END',\n",
       " '# i h e ar d b ow w ow END',\n",
       " 'b l d _ 6 0 0 _ k w h END',\n",
       " 'b l d _ 6 5 0 _ k w h END',\n",
       " 's h e e eEND',\n",
       " '# f m 2 0 1 1 END',\n",
       " '- s h eEND',\n",
       " '- h eEND',\n",
       " 's / h eEND',\n",
       " 's h e / h eEND',\n",
       " '- i tEND',\n",
       " 's h e eEND',\n",
       " 'h e / s h eEND',\n",
       " 'h eEND',\n",
       " 's h eEND',\n",
       " '< U R L - i . v e >END',\n",
       " \"l ' v eEND\",\n",
       " 'th e y v END',\n",
       " \"you \\\\ ' v eEND\",\n",
       " \"i ' b eEND\",\n",
       " '< U R L - i h e ar t m o v i e s . or g >END',\n",
       " 'i w ou l d aEND',\n",
       " 'w e ` v eEND',\n",
       " \"i ha v en ' tEND\",\n",
       " '# i ha v en e v erEND',\n",
       " \"a : i ' v eEND\",\n",
       " 'w e v END',\n",
       " 'w e ´ v eEND',\n",
       " \"y u ' v eEND\",\n",
       " 'u ’ v eEND',\n",
       " \"- i ' v eEND\",\n",
       " \"th e re ' v eEND\",\n",
       " \"i ' d aEND\",\n",
       " '< U R L - v o om a x er . c om >END',\n",
       " \"th a t ' v eEND\",\n",
       " \"w e ' v END\",\n",
       " 'you ´ v eEND',\n",
       " 'i v e eEND',\n",
       " \"i ' d ' v eEND\",\n",
       " 'you ` v eEND',\n",
       " \"i \\\\ ' v eEND\",\n",
       " 'you v END',\n",
       " \"you ' v END\",\n",
       " '# n e v er ha v e i e v erEND',\n",
       " \"u ' v END\",\n",
       " '< U R L - n a u g h t y d o g . c om >END',\n",
       " 'i ha v en tEND',\n",
       " \"i v ' eEND\",\n",
       " 'th e y ’ v eEND',\n",
       " '# ha v e you e v erEND',\n",
       " 'i ´ v eEND',\n",
       " 'i ` v eEND',\n",
       " '# ha v e u e v erEND',\n",
       " 'th e y v eEND',\n",
       " \"i ' v END\",\n",
       " '0 . 0 0 % END',\n",
       " 'w e v eEND',\n",
       " 'u v eEND',\n",
       " 'w e ’ v eEND',\n",
       " \"u ' v eEND\",\n",
       " 'you ’ v eEND',\n",
       " 'you v eEND',\n",
       " 'i ’ v eEND',\n",
       " \"th e y ' v eEND\",\n",
       " \"i ' v eEND\",\n",
       " 'i v eEND',\n",
       " \"you ' v eEND\",\n",
       " \"w e ' v eEND\",\n",
       " 'n o o o o o o tEND',\n",
       " 'n o t t t t t t tEND',\n",
       " 'n o h tEND',\n",
       " 'you ha v eEND',\n",
       " '/ n o t / END',\n",
       " 'n o i tEND',\n",
       " '- n o t - END',\n",
       " 'n o t t t t t tEND',\n",
       " 'n o o o o o tEND',\n",
       " \"n ' tEND\",\n",
       " 'n n o tEND',\n",
       " 'n a h tEND',\n",
       " '_ n o t _ END',\n",
       " 'd e s er v e d l yEND',\n",
       " 'n o t t t t tEND',\n",
       " 'n o o o o tEND',\n",
       " 'n o t - END',\n",
       " 'n o o o tEND',\n",
       " 'n o o tEND',\n",
       " 'n toEND',\n",
       " 'n o t t t tEND',\n",
       " 'n o t t tEND',\n",
       " 'r i g h t f u l l yEND',\n",
       " 'n a w tEND',\n",
       " 'n 0 tEND',\n",
       " 'n o t tEND',\n",
       " 'n o tEND',\n",
       " 'n tEND',\n",
       " 'g o t t n END',\n",
       " 'b 3 3 n END',\n",
       " 'b e e e e e en END',\n",
       " 'g o t t on END',\n",
       " 's u c c e s s f u l yEND',\n",
       " 'b e e e e en END',\n",
       " 'b e en n END',\n",
       " 'u n d er g on eEND',\n",
       " 'b e e e en END',\n",
       " 'b e e en END',\n",
       " 'b e en END',\n",
       " 'g o t t en END',\n",
       " 'j u x tEND',\n",
       " '/ / j u s tEND',\n",
       " 'j u st t t t tEND',\n",
       " 'j x tEND',\n",
       " '# s p or c l eEND',\n",
       " 'j st tEND',\n",
       " 'j u r tEND',\n",
       " 'j y sEND',\n",
       " '/ j u s tEND',\n",
       " '- j u sEND',\n",
       " 'j y s tEND',\n",
       " 'd d e u b e lEND',\n",
       " 'j u st t t tEND',\n",
       " 'j u s s s s tEND',\n",
       " 'j u $ tEND',\n",
       " 'j u u sEND',\n",
       " 'j s s tEND',\n",
       " 'j u u u u u s tEND',\n",
       " 'k u s tEND',\n",
       " 'j h u sEND',\n",
       " 'j u s s s sEND',\n",
       " 'j h u s sEND',\n",
       " 'j u st sEND',\n",
       " 'j u s s s tEND',\n",
       " 'j u s x END',\n",
       " 'j u x x END',\n",
       " 'j z tEND',\n",
       " 'j u z z END',\n",
       " 'j u d tEND',\n",
       " '< U R L - w o o . l y >END',\n",
       " 'j u h sEND',\n",
       " 'j u u u u s tEND',\n",
       " 'j j u s tEND',\n",
       " 'j u z tEND',\n",
       " 'j u st t tEND',\n",
       " 'j u st e dEND',\n",
       " 'j u s rEND',\n",
       " 'j u u u s tEND',\n",
       " 'j u t sEND',\n",
       " 'j u s yEND',\n",
       " 'j u u s tEND',\n",
       " 'j u a tEND',\n",
       " 'j u s z END',\n",
       " '# j u s tEND',\n",
       " 'j s sEND',\n",
       " 'j u s s sEND',\n",
       " '< U R L - b u y t t er . c om >END',\n",
       " 'j u s s tEND',\n",
       " 'j z END',\n",
       " '- j u s tEND',\n",
       " '# d on t a c t l i k e you n e v erEND',\n",
       " 'j u tEND',\n",
       " 'j u st tEND',\n",
       " 'j u x END',\n",
       " 'j s u tEND',\n",
       " 'j u z END',\n",
       " 'j s tEND',\n",
       " 'j u s sEND',\n",
       " 'j u s tEND',\n",
       " 'j u sEND',\n",
       " \"a in \\\\ ' tEND\",\n",
       " 'a in ´ tEND',\n",
       " \"a ' in tEND\",\n",
       " 'a it n END',\n",
       " 'a in yEND',\n",
       " 'w u s z END',\n",
       " 'a y n tEND',\n",
       " 'a in n tEND',\n",
       " 'i an tEND',\n",
       " \"an ' tEND\",\n",
       " 'a in eEND',\n",
       " 'a in ` tEND',\n",
       " 'a in n END',\n",
       " 'a in t tEND',\n",
       " 'a i in tEND',\n",
       " 'i a in tEND',\n",
       " 'a in ’ tEND',\n",
       " 'an i tEND',\n",
       " 'a in END',\n",
       " 'a in tEND',\n",
       " \"a in ' tEND\",\n",
       " 's h ou d aEND',\n",
       " 's h ou l d n aEND',\n",
       " 's h ou l aEND',\n",
       " \"w ou l d n ' t ' v eEND\",\n",
       " \"s h ou l d n ' t ' v eEND\",\n",
       " 's h ou l d v END',\n",
       " \"s h u d ' v eEND\",\n",
       " 'h an tEND',\n",
       " \"ha v ' n tEND\",\n",
       " 'c l d aEND',\n",
       " 'w l d v eEND',\n",
       " \"s h ou l d ' aEND\",\n",
       " 'w ou l d a aEND',\n",
       " 's h ou l d d aEND',\n",
       " 'w u l d v eEND',\n",
       " \"w u d ' v eEND\",\n",
       " 'w ou l d d aEND',\n",
       " 's h u l d v eEND',\n",
       " 's h ou l d a aEND',\n",
       " \"ha v e ' tEND\",\n",
       " 'c ou l d ’ v eEND',\n",
       " 'ha v en t tEND',\n",
       " 's h l d v eEND',\n",
       " 'c u d v eEND',\n",
       " \"m a y ' v eEND\",\n",
       " \"h v n ' tEND\",\n",
       " 'w ou l d ’ v eEND',\n",
       " 'a v n tEND',\n",
       " 'w l d aEND',\n",
       " 's h ou l d ’ v eEND',\n",
       " 'c u l d aEND',\n",
       " 'ha v en ´ tEND',\n",
       " 's h l d aEND',\n",
       " 'm i g h t v eEND',\n",
       " 'ha v en ` tEND',\n",
       " 'ha d n ’ tEND',\n",
       " '# g l o c al u r b an END',\n",
       " 'h v en tEND',\n",
       " 's h u d v eEND',\n",
       " 'w u d v eEND',\n",
       " \"ha v e ' n tEND\",\n",
       " 'c u d d aEND',\n",
       " 'm i g h t aEND',\n",
       " 'w u l d aEND',\n",
       " 's h u l d aEND',\n",
       " 'w u d d aEND',\n",
       " 's h u d d aEND',\n",
       " 'w u d aEND',\n",
       " 's h u d aEND',\n",
       " 'm u st v eEND',\n",
       " 'h v n tEND',\n",
       " \"m i g h t ' v eEND\",\n",
       " 'ha d n tEND',\n",
       " \"ha v n ' tEND\",\n",
       " 'ha v en ’ tEND',\n",
       " 'c ou l d v eEND',\n",
       " 'm u st aEND',\n",
       " \"m u st ' v eEND\",\n",
       " 'w ou l d v eEND',\n",
       " 's h ou l d v eEND',\n",
       " 'ha v n tEND',\n",
       " 'c ou l d aEND',\n",
       " \"c ou l d ' v eEND\",\n",
       " 'w ou l d aEND',\n",
       " \"ha d n ' tEND\",\n",
       " \"s h ou l d ' v eEND\",\n",
       " \"w ou l d ' v eEND\",\n",
       " 's h ou l d aEND',\n",
       " \"ha v en ' tEND\",\n",
       " 'ha v en tEND',\n",
       " 'n e v v aEND',\n",
       " 'n e e e e v erEND',\n",
       " 'n e v e tEND',\n",
       " 'n e e e v erEND',\n",
       " 'en v erEND',\n",
       " 'n er v erEND',\n",
       " 'n e e v erEND',\n",
       " 'n e v a a aEND',\n",
       " 'b e v erEND',\n",
       " '# in e v erEND',\n",
       " 'g l a d yEND',\n",
       " 'n e v e erEND',\n",
       " '- n e v erEND',\n",
       " \"n e ' erEND\",\n",
       " 'l e t c h aEND',\n",
       " 'l e t c h u END',\n",
       " 'n e v er r r rEND',\n",
       " 'n v aEND',\n",
       " 'n e v a h END',\n",
       " 'n e v a aEND',\n",
       " 'n e v er r rEND',\n",
       " 'n v erEND',\n",
       " 'n e v er rEND',\n",
       " '# n e v erEND',\n",
       " 'n e v rEND',\n",
       " 'g l a d l yEND',\n",
       " 'n v rEND',\n",
       " 'n e v erEND',\n",
       " 'n e v aEND',\n",
       " 'e v u rEND',\n",
       " 'e v a a a a aEND',\n",
       " 'e v e aEND',\n",
       " 'e v e e e erEND',\n",
       " 'e v er r r r r r r r rEND',\n",
       " 'e v er r r r r r r rEND',\n",
       " 'e v e e erEND',\n",
       " 'e v a a a aEND',\n",
       " 'e v e erEND',\n",
       " 'n e v a rEND',\n",
       " 'e v er r r r r r rEND',\n",
       " 'e v a a aEND',\n",
       " 'e v a aEND',\n",
       " 'e v er r r r r rEND',\n",
       " 'e v er r r r rEND',\n",
       " 'e v a h END',\n",
       " 'e v er r r rEND',\n",
       " 'e v er rEND',\n",
       " 'e v er r rEND',\n",
       " 'e v rEND',\n",
       " 'e v a rEND',\n",
       " 'e v aEND',\n",
       " 'e v erEND',\n",
       " 'on l eEND',\n",
       " 'in l yEND',\n",
       " 'on l e eEND',\n",
       " 'on l u END',\n",
       " 'on y lEND',\n",
       " 'on l l yEND',\n",
       " 'on l tEND',\n",
       " 'on l y y yEND',\n",
       " 'o l n yEND',\n",
       " '- on l yEND',\n",
       " '0 n l yEND',\n",
       " 'on l i iEND',\n",
       " 'on yEND',\n",
       " 'on l y yEND',\n",
       " 'on l yEND',\n",
       " 'on l iEND',\n",
       " 'g e t 2 END',\n",
       " 'n e c c e s ar i l yEND',\n",
       " 'n e c c e s s ar i l yEND',\n",
       " 'e e e m END',\n",
       " 'e v er n END',\n",
       " 'n e v en END',\n",
       " 'l e t e m END',\n",
       " 'e v en n n END',\n",
       " 'e v e b END',\n",
       " 'e e en END',\n",
       " 'e v e m END',\n",
       " 'e v e en END',\n",
       " '< U R L - g t p 1 2 3 . c om >END',\n",
       " '- e v en END',\n",
       " \"1 0 x ' sEND\",\n",
       " \"m a k e ' e m END\",\n",
       " \"l e t ' e m END\",\n",
       " 'e v en n END',\n",
       " 'e e m END',\n",
       " 'e v n END',\n",
       " 'e v en END',\n",
       " 'n e c e s s ar i l yEND',\n",
       " 're e e e e e e al l yEND',\n",
       " 're a a al yEND',\n",
       " 're al l l l l l l l l l yEND',\n",
       " 'r l iEND',\n",
       " 're al l i eEND',\n",
       " 're al l l l l y y yEND',\n",
       " 're e l iEND',\n",
       " 'r 3 al l yEND',\n",
       " 're al l l l y yEND',\n",
       " 're al l y - re al l yEND',\n",
       " 're l al yEND',\n",
       " 're al l l l y y y yEND',\n",
       " 'r i l l iEND',\n",
       " 're al l y re al l y re al l yEND',\n",
       " '- re al l y - END',\n",
       " 're al l y y y y y yEND',\n",
       " 're a al l y yEND',\n",
       " 'e al l yEND',\n",
       " 're e e a a al l yEND',\n",
       " 're al l y re al l yEND',\n",
       " 're e a al l yEND',\n",
       " 'r re al l yEND',\n",
       " 're a a a al l l yEND',\n",
       " 're al l u END',\n",
       " 're a a a a a al l yEND',\n",
       " '/ re al l y / END',\n",
       " 're al y yEND',\n",
       " 're a al l l yEND',\n",
       " 're al l l l l l l l l yEND',\n",
       " 're al l l l y y yEND',\n",
       " 're a a al l l yEND',\n",
       " 'w e al l yEND',\n",
       " 're e e e e e al l yEND',\n",
       " 're al l l y y y yEND',\n",
       " 're l l iEND',\n",
       " 'g en u in l yEND',\n",
       " 're al l tEND',\n",
       " 're al l i iEND',\n",
       " 're al l l y yEND',\n",
       " 're a al yEND',\n",
       " 're al l l l l l l l yEND',\n",
       " '_ re al l y _ END',\n",
       " 're al l y y y y yEND',\n",
       " 're al l y 2 END',\n",
       " 's h o l eEND',\n",
       " 're a a a a al l yEND',\n",
       " 're e l yEND',\n",
       " 're l l eEND',\n",
       " 're al l l y y yEND',\n",
       " 's h o lEND',\n",
       " 're e al l yEND',\n",
       " 're e e e e al l yEND',\n",
       " 're al l l l l l l yEND',\n",
       " 'r i l l yEND',\n",
       " 're al l y y y yEND',\n",
       " 're a a a al l yEND',\n",
       " 're a a al l yEND',\n",
       " 'r i l iEND',\n",
       " 're e e al l yEND',\n",
       " 're a al l yEND',\n",
       " 're al l l l l l yEND',\n",
       " 're e e e al l yEND',\n",
       " 're al l y y yEND',\n",
       " 'r i l yEND',\n",
       " 's h o l lEND',\n",
       " 're al iEND',\n",
       " 're l iEND',\n",
       " 're al l l l l yEND',\n",
       " 're l l yEND',\n",
       " 're al l iEND',\n",
       " 're l eEND',\n",
       " 're al l y yEND',\n",
       " 're al l l l yEND',\n",
       " 're al l l yEND',\n",
       " 'r l l yEND',\n",
       " 'g en u in e l yEND',\n",
       " 're al yEND',\n",
       " 're al l yEND',\n",
       " 'r l yEND',\n",
       " 'al re d a yEND',\n",
       " 'al re a y dEND',\n",
       " 'f in al iEND',\n",
       " 'o f f i s h END',\n",
       " 'al r a d yEND',\n",
       " 'w o o d aEND',\n",
       " 'o re d iEND',\n",
       " 'al re a a d yEND',\n",
       " 'al re a d y y y y yEND',\n",
       " 'al re a d d yEND',\n",
       " 'al e a d yEND',\n",
       " 'al re a yEND',\n",
       " 's u c e s s f u l l yEND',\n",
       " 'al re d iEND',\n",
       " 'a re a d yEND',\n",
       " 'al re a d iEND',\n",
       " 'al re a d i iEND',\n",
       " 'al re a dEND',\n",
       " 'al re a d y y yEND',\n",
       " 'a w re a d yEND',\n",
       " 'al r dEND',\n",
       " 'c u d aEND',\n",
       " 'al re d yEND',\n",
       " 'al l re a d yEND',\n",
       " 'al re a d y yEND',\n",
       " 'al r d yEND',\n",
       " 'p re v i ou s l yEND',\n",
       " 'al re a d yEND',\n",
       " 're c en t l yEND',\n",
       " '- al m o s tEND',\n",
       " 'n e al yEND',\n",
       " 'n e ar l l yEND',\n",
       " 'al m 0 s tEND',\n",
       " 'al om o s tEND',\n",
       " 'al om s tEND',\n",
       " 'al m o st tEND',\n",
       " 'al m s o tEND',\n",
       " 'a m o s tEND',\n",
       " 'al l m o s tEND',\n",
       " 'al m s tEND',\n",
       " 'a v er a g ingEND',\n",
       " 'r ou g h l yEND',\n",
       " 'v i r t u al l yEND',\n",
       " 'a p p r o x i m a t e l yEND',\n",
       " 'p r a c t i c al l yEND',\n",
       " 'al m o s tEND',\n",
       " 'n e ar l yEND',\n",
       " 's i b b yEND',\n",
       " 'c u r re n t yEND',\n",
       " 'o f f i c a i l l yEND',\n",
       " 'o f f c i al l yEND',\n",
       " '# b g g p l a yEND',\n",
       " 'c u re n t l yEND',\n",
       " 'b u s i l yEND',\n",
       " 'o f f i c al yEND',\n",
       " 'o f i c i al l yEND',\n",
       " 'c or d i al l yEND',\n",
       " '< U R L - l i st en . g h e t t or a d i o . f m >END',\n",
       " '< U R L - k a i s e re g g . c h >END',\n",
       " 'h u c k l e b er r i e sEND',\n",
       " 'h e i s eEND',\n",
       " '# re a d c a s tEND',\n",
       " 'p re s en t l yEND',\n",
       " 'o f f i c i al yEND',\n",
       " '< U R L - g o . n i k e . c om >END',\n",
       " 'o f f i c al l yEND',\n",
       " 're p or t e d l yEND',\n",
       " 'o f f i c i al l yEND',\n",
       " 'c u r re n t l yEND',\n",
       " 'f in al l l y yEND',\n",
       " 'f in al l iEND',\n",
       " 'f in al l l y y yEND',\n",
       " '# o f f i c i al l yEND',\n",
       " 'f i i i in al l yEND',\n",
       " 'f n al l yEND',\n",
       " 'f in al l y y y y yEND',\n",
       " 'f i in al l yEND',\n",
       " '- f in al l yEND',\n",
       " 'b er l yEND',\n",
       " 'f in al l l l l l yEND',\n",
       " '# th in g s i d i d o v er th e s u m m erEND',\n",
       " '< U R L - m a p m y f it n e s s . c om >END',\n",
       " 'f in al l l l l yEND',\n",
       " 'f in i al l yEND',\n",
       " 's n a c k f e e dEND',\n",
       " 'f in al l y y y yEND',\n",
       " 'f in al l l l yEND',\n",
       " 'f i an l l yEND',\n",
       " '< U R L - m a p m y r i d e . c om >END',\n",
       " 'f in al l y y yEND',\n",
       " 's u c c e s f u l l yEND',\n",
       " 'f in al l y yEND',\n",
       " '# m y f it n e s s p a lEND',\n",
       " 'f in n al l yEND',\n",
       " '< U R L - m a p m y r u n . c om >END',\n",
       " 'f in al l l yEND',\n",
       " 're l u c t an t l yEND',\n",
       " 'f in n al yEND',\n",
       " '< U R L - c o or d . in f o >END',\n",
       " 'f in al yEND',\n",
       " 'f in al l yEND',\n",
       " 's u c c e s s f u l l yEND',\n",
       " 'k n ow e s tEND',\n",
       " 'r a th rEND',\n",
       " 'c an s tEND',\n",
       " '< U R L - s u p er m ar k e t . c om >END',\n",
       " 'r a th aEND',\n",
       " 'r a th erEND',\n",
       " 's ha l tEND',\n",
       " \"d on t ' c h aEND\",\n",
       " 'i i on END',\n",
       " 'i m m oEND',\n",
       " '4 + 4 END',\n",
       " 'i i b END',\n",
       " 'i d on t tEND',\n",
       " 'b r in j a lEND',\n",
       " 'i d on END',\n",
       " 'th e l lEND',\n",
       " '2 iEND',\n",
       " 'n u s tEND',\n",
       " 'a b t aEND',\n",
       " 'l e y sEND',\n",
       " 'm e + you END',\n",
       " 'l e m m aEND',\n",
       " 's h ou l d sEND',\n",
       " 'w e l l iEND',\n",
       " 'i / i iEND',\n",
       " 'd i d s tEND',\n",
       " '1 iEND',\n",
       " 'c ha r s e tEND',\n",
       " 'u l dEND',\n",
       " 'd / n END',\n",
       " 'í END',\n",
       " 'f . i . n . a . l . s .END',\n",
       " 'i ow n END',\n",
       " 'i i dEND',\n",
       " 'n on - v i r g in END',\n",
       " 's k y ha w kEND',\n",
       " 's k y l an eEND',\n",
       " 'c h _ t y p eEND',\n",
       " 'k en o tEND',\n",
       " 'd in n yEND',\n",
       " '# i d on tEND',\n",
       " '- i iEND',\n",
       " '# y a m a m a e v erEND',\n",
       " '2 0 1 0 / 0 7 END',\n",
       " '2 0 1 0 / 0 5 END',\n",
       " 'c . l . a . s . s .END',\n",
       " '& iEND',\n",
       " 'i f u END',\n",
       " 'f m tEND',\n",
       " 'th a t iEND',\n",
       " 'l e m iEND',\n",
       " '# m y g o al f or 2 0 1 2 END',\n",
       " 'y u dEND',\n",
       " 'e b u END',\n",
       " 'b o t t aEND',\n",
       " 'm on e y - b a c kEND',\n",
       " '1 9 tEND',\n",
       " 'i 8 END',\n",
       " '# on l y f a t p e o p l eEND',\n",
       " '# th in g s i a in t d on e y e tEND',\n",
       " 'd on t c h u END',\n",
       " 's g eEND',\n",
       " 'i f iEND',\n",
       " 'i l dEND',\n",
       " '# c on f u s in g th in g s g i r l s d oEND',\n",
       " '_ i _ END',\n",
       " 'm u z END',\n",
       " 'c an iEND',\n",
       " '# u r g i r l f r i en d e v erEND',\n",
       " 'i - i - iEND',\n",
       " '# on l y u g l y p e o p l eEND',\n",
       " '2 0 1 0 / 1 2 END',\n",
       " 'l e t t sEND',\n",
       " 'n e erEND',\n",
       " '2 0 1 0 / 1 0 END',\n",
       " '2 0 1 0 / 0 4 END',\n",
       " 'c y a aEND',\n",
       " 's on tEND',\n",
       " '2 0 1 0 / 0 8 END',\n",
       " 'd on n END',\n",
       " '2 0 1 0 / 0 6 END',\n",
       " 'a p t - g e tEND',\n",
       " 'u on END',\n",
       " 'd o an END',\n",
       " 'd o s tEND',\n",
       " '# on l y w h it e p e o p l eEND',\n",
       " 'i i i iEND',\n",
       " '# th in g s b l a c k p e o p l e d oEND',\n",
       " 'i d w END',\n",
       " '2 + 2 END',\n",
       " 'in eEND',\n",
       " 'ha s tEND',\n",
       " 'i d i d n tEND',\n",
       " '1 + 1 END',\n",
       " 'h e dEND',\n",
       " 'p r o v o k ingEND',\n",
       " 'i d n tEND',\n",
       " 'u lEND',\n",
       " 'u m aEND',\n",
       " 'w dEND',\n",
       " 'u dEND',\n",
       " 'i i iEND',\n",
       " 'l lEND',\n",
       " 'i v END',\n",
       " 'i on END',\n",
       " 'i iEND',\n",
       " 'i dEND',\n",
       " 'p r a c t i c al yEND',\n",
       " 'u s s u al l yEND',\n",
       " 'a c c i d en t i al l yEND',\n",
       " 's u d en l yEND',\n",
       " 't e ar f u l l yEND',\n",
       " 'u s u al l l yEND',\n",
       " 'n a i v e l yEND',\n",
       " 're f l e x i v e l yEND',\n",
       " 'o p t i on al l yEND',\n",
       " 'l i k e 2 END',\n",
       " 'c on t in u o s l yEND',\n",
       " 'u n w i s e l yEND',\n",
       " 'p r a c t i c l yEND',\n",
       " 'a t u al l yEND',\n",
       " 'a c t l yEND',\n",
       " 'p u r p o s l yEND',\n",
       " 's e c re t e l yEND',\n",
       " 'ha r d l e yEND',\n",
       " 'a u t om a t i c al yEND',\n",
       " 'c on c e i v a b l yEND',\n",
       " 'a b s en t m in d e d l yEND',\n",
       " 'a c t u al l iEND',\n",
       " 'l it r al l yEND',\n",
       " 's u b c on c i ou s l yEND',\n",
       " 'c on st an l yEND',\n",
       " 'l it er l yEND',\n",
       " 'd o in tEND',\n",
       " 'm a k e m END',\n",
       " 's u p p o s e l yEND',\n",
       " 'a c t u al l y yEND',\n",
       " 'a c t u l yEND',\n",
       " 'g o an n aEND',\n",
       " 'b a s c i al l yEND',\n",
       " 'p r a t i c al l yEND',\n",
       " 'i i v eEND',\n",
       " 's u p p o s a b l yEND',\n",
       " 'b a s i c al yEND',\n",
       " 'er r on e ou s l yEND',\n",
       " 'f l a t l yEND',\n",
       " 'c a s j END',\n",
       " 'l e g it l yEND',\n",
       " 'l it t er l yEND',\n",
       " 'm u s s yEND',\n",
       " 'or g in al l yEND',\n",
       " 'd on rEND',\n",
       " 'in a d v er t an t l yEND',\n",
       " 'a c t al l yEND',\n",
       " 'a c t u al iEND',\n",
       " 'u s u s al l yEND',\n",
       " 'a c c u al l yEND',\n",
       " 'in t u it i v e l yEND',\n",
       " 'a u t om a t i c l yEND',\n",
       " 'g r u d g in g l yEND',\n",
       " 'b e g r u d g in g l yEND',\n",
       " 's c ar c e l yEND',\n",
       " 'b ar l yEND',\n",
       " 'l it t er al yEND',\n",
       " 'b l a t en t l yEND',\n",
       " 'ha b it u al l yEND',\n",
       " 'or d in ar i l yEND',\n",
       " 'a f f e c t i on a t e l yEND',\n",
       " 's n e a k i l yEND',\n",
       " 'a c c i d en t al yEND',\n",
       " 'n or m al yEND',\n",
       " 's in g l e h an d e d l yEND',\n",
       " 'c om p u l s i v e l yEND',\n",
       " 's u b l i m in al l yEND',\n",
       " 'in v o l u n t ar i l yEND',\n",
       " 'a c t u al l l yEND',\n",
       " 'g in eEND',\n",
       " 'd e f in it i v e l yEND',\n",
       " 'u s u al yEND',\n",
       " 'l it er al yEND',\n",
       " 'b r a v e l yEND',\n",
       " 'a c c t u al l yEND',\n",
       " 'a c u al l yEND',\n",
       " 'b a s i c l yEND',\n",
       " 'ha f fEND',\n",
       " 'in st in c t i v e l yEND',\n",
       " 'u n c on s c i ou s l yEND',\n",
       " 's in g l e - h an d e d l yEND',\n",
       " 's l y l yEND',\n",
       " 'a c t a u l l yEND',\n",
       " 'd r u n k en l yEND',\n",
       " 'a c u t al l yEND',\n",
       " 'f o o l i s h l yEND',\n",
       " 'b e ar l yEND',\n",
       " 'p u r p o s e f u l l yEND',\n",
       " 'j o k in g l yEND',\n",
       " 'r ou t in e l yEND',\n",
       " 'k n ow in g l yEND',\n",
       " 's u b c on s c i ou s l yEND',\n",
       " 'u n k n ow in g l yEND',\n",
       " 'a c t u l l yEND',\n",
       " 'm i r a c u l ou s l yEND',\n",
       " 'l it t er al l yEND',\n",
       " 'c o l l e c t i v e l yEND',\n",
       " 't r a d it i on al l yEND',\n",
       " 'in a d v er t en t l yEND',\n",
       " 'm i st a k en l yEND',\n",
       " 'v o l u n t ar i l yEND',\n",
       " 'b l in d l yEND',\n",
       " 'in d i re c t l yEND',\n",
       " 's p on t an e ou s l yEND',\n",
       " 'w i l l in g l yEND',\n",
       " 'h e re b yEND',\n",
       " 'g r a d u al l yEND',\n",
       " 'a c t u al yEND',\n",
       " 'j e sEND',\n",
       " 'd e l i b er a t e l yEND',\n",
       " 'c on t in u ou s l yEND',\n",
       " 's e l d om END',\n",
       " 'in t en t i on al l yEND',\n",
       " 'p u r p o s e l yEND',\n",
       " 'b ar l e yEND',\n",
       " 'in it i al l yEND',\n",
       " 'a c t i v e l yEND',\n",
       " 'on tEND',\n",
       " 'c a s u al l yEND',\n",
       " 'e s s en t i al l yEND',\n",
       " 'p r ou d l yEND',\n",
       " 't y p i c al l yEND',\n",
       " 'a c c i d en t l yEND',\n",
       " 'm a g i c al l yEND',\n",
       " 'al l e g e d l yEND',\n",
       " 's u p p o s e d l yEND',\n",
       " 'or i g in al l yEND',\n",
       " 's e c re t l yEND',\n",
       " 'g en er al l yEND',\n",
       " 'r a re l yEND',\n",
       " 'a u t om a t i c al l yEND',\n",
       " 'a c c i d en t al l yEND',\n",
       " 'r an d om l yEND',\n",
       " 'n or m al l yEND',\n",
       " 'c on st an t l yEND',\n",
       " 'ha r d l yEND',\n",
       " 'b a re l yEND',\n",
       " 'b a s i c al l yEND',\n",
       " 'l it er al l yEND',\n",
       " 'a c t u al l yEND',\n",
       " 'u s u al l yEND',\n",
       " 's u d d en t l yEND',\n",
       " '- al w a y sEND',\n",
       " 'al o sEND',\n",
       " 'c o in c i d en t l yEND',\n",
       " 'p r a y er f u l l yEND',\n",
       " 'd e m on b r u en END',\n",
       " 'd e s p ar a t e l yEND',\n",
       " 'g i e sEND',\n",
       " 'd e s p er a t l yEND',\n",
       " 'a s l oEND',\n",
       " 'al t er n a t e l yEND',\n",
       " 'b e l a t e d l yEND',\n",
       " 'j x END',\n",
       " 's u b s e q u en t l yEND',\n",
       " 's in c er l yEND',\n",
       " 'h en c e f or th END',\n",
       " 'u l t i m a t e l yEND',\n",
       " 'd e s p er a t e l yEND',\n",
       " 's in c e re l yEND',\n",
       " 's u d d en l yEND',\n",
       " 'al s oEND',\n",
       " 'o b i ou s l yEND',\n",
       " 'd e a d a z z END',\n",
       " 'e v i d en t al l yEND',\n",
       " 't o t al l y y yEND',\n",
       " 's r i ou s l yEND',\n",
       " 'o b v i ou l s yEND',\n",
       " 'o b v s l yEND',\n",
       " 's er o i u s l yEND',\n",
       " 'n e v er r r r r r rEND',\n",
       " 's er i s ou l yEND',\n",
       " 'h on e st l y yEND',\n",
       " 's r l s yEND',\n",
       " 's er i ou s l y y y yEND',\n",
       " 'l it er al l l yEND',\n",
       " 'o b v i o s l yEND',\n",
       " 'o v i ou s l yEND',\n",
       " 'g o t t c h aEND',\n",
       " 'o b v z END',\n",
       " 'd / aEND',\n",
       " 'w i s h iEND',\n",
       " 'g o t c ha aEND',\n",
       " 'i i i i i i i i iEND',\n",
       " '# j u st c a u s e w e c o o lEND',\n",
       " \"s or r y ' sEND\",\n",
       " 'l i k e e e e e eEND',\n",
       " 's h e e e eEND',\n",
       " 'g o t c h y aEND',\n",
       " 's r l yEND',\n",
       " 'f er re a lEND',\n",
       " 's er i ou s l y y yEND',\n",
       " 's i r i u s l yEND',\n",
       " 'g e z END',\n",
       " 'n e v er r r r r rEND',\n",
       " 'l o k e yEND',\n",
       " 's u r l e yEND',\n",
       " 's er i ou s l l yEND',\n",
       " 'h on e s l t yEND',\n",
       " 's er i u o s l yEND',\n",
       " 'd e a d a s s sEND',\n",
       " 'l ow k e y yEND',\n",
       " '- re al l yEND',\n",
       " 's er i u s l yEND',\n",
       " 's er i ou l yEND',\n",
       " 'b e t c h u END',\n",
       " 's er ou s l yEND',\n",
       " 's er z l yEND',\n",
       " 'h i g h k e yEND',\n",
       " 'n e v er r r r rEND',\n",
       " 's er i o s l yEND',\n",
       " 'l i k e e e e eEND',\n",
       " 's er i ou l s yEND',\n",
       " 's er i o s u l yEND',\n",
       " 'p er s on al yEND',\n",
       " 'u n d er st an d a b l yEND',\n",
       " 'u n n oEND',\n",
       " 's er i ou s l y yEND',\n",
       " 'g e d d i tEND',\n",
       " 'l i k e e e eEND',\n",
       " 'th e o re t i c al l yEND',\n",
       " 'd . aEND',\n",
       " 're al i st i c al l yEND',\n",
       " 'o b v iEND',\n",
       " 't r u th f u l l yEND',\n",
       " 'o b v sEND',\n",
       " 'b e t c h aEND',\n",
       " '# l ow k e yEND',\n",
       " 'o b v END',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preform_bpe(brown_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9888b25499797c4fb0fd4f13646b0c3c",
     "grade": false,
     "grade_id": "cell-7d1e49878db56df4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Jakie angielskie słowo jako pierwsze dostało swój własny token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df4c7b8b5aa2b077eaa2d42429797139",
     "grade": true,
     "grade_id": "cell-acd48c77e2c1bcec",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd51e6fc0cd1d3b4d8b9e9a2fa1b0316",
     "grade": false,
     "grade_id": "cell-df60f5e5c6fe4ca0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Jakie są zalety korzystania z tokenizacji BPE w kontekście tworzenia reprezentacji (problem OOV, odnieś się do  k-gramów i n-gramów)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64306e36b58f1eee12c8bb339123e105",
     "grade": true,
     "grade_id": "cell-006ef6fd3e397206",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Możemy zaobserwować następujące zalety:\n",
    "* kompresja danych dzięki redukcji powtarzających się ciągów znaków do specjalnych symboli. Dodatkowo istnieje przekształcenie odwrotne więc operacja nie będzie stratna.\n",
    "* zapewnia że popularne słowa będą reprezentowane jako jeden token, podczas gdy mniej popularne będą powstawać ze zlepień kilku tokenów\n",
    "* w przypadku k-gramów reprezentuje najpopularniejsze jako osobne tokeny, umożliwiając jednocześnie łączenie tokenów tworząc dłuższe ciągi znaków\n",
    "* moglibyśmy go wykorzystać do n-gramów gdzie zamiast łączenia poszczególnych znaków dokonywalibyśmy łączenia całych wyrazów tworząc częste podsekwencje (złączenie kilku tokenów reprezentujących słowa w jeden token)\n",
    "* radzi sobie z problemem OOV poprzez rozdzielanie na fragmenty, które zna tj. ma możliwość zastąpienia fragmentu słowa tokenem X a resztę innym tokenem (bądź też pozostawi bez zamiany) co pozwala potencjalnie reprezentować nieznane słowo znanymi wcześniej fragmentami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - klasyfikacja (15 pkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod powinien wczytać i ztokenizować zbiór danych dot. analizy wydźwięku. Jeśli nie masz biblioteki `nltk` musisz ją zainstalować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data set ['tweets.txt']\n"
     ]
    }
   ],
   "source": [
    "from helpers import DataSet\n",
    "training_set = DataSet(['tweets.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajdziesz przykład odczytu jednego tweeta z obiektu DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\n",
      "['dear', '@microsoft', 'the', 'newooffice', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'lync', 'update', '?', \"c'mon\", '.']\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "for i in training_set.tweets:\n",
    "    print(i.text)\n",
    "    print(i.tokens)\n",
    "    print(i.clazz)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemy IL często pracują z bardzo dużą liczbą cech, które są rzadkie np. cechy Bag-Of-Words, cechy n-gramowe itd. Powoduje to że klasyczna macierz przykłady uczące na cechy rośnie do bardzo dużych rozmiarów nawet dla małych zbiorów uczących (w sensie liczby przykładów). Ponadto samo przechowywanie w pamięci słownika mapującego konkretne słowa/n-gramy na indeksy kolumn macierzy może być bardzo kosztowne pamięciowo przy dużych rozmiarach słownika.\n",
    "\n",
    "Istnieje jednak technika, która pozwala nam na ominięcie tej przeszkody: haszowanie cech. Opis tej techniki znajdziesz na stronie:  https://en.wikipedia.org/wiki/Feature_hashing Jest ona też implementowana w obiekcie `sklearn.feature_extraction.FeatureHasher`. Zapoznaj się z opisem techniki i wykonaj poniższe polecenia.\n",
    "\n",
    "- Wykorzystując haszowanie cech wytrenuj wybrany klasyfikator na zbiorze uczącym dla cech Bag-of-words (możesz też spróbować cechy n-gramowe). Możesz wykorzystać gotową tokenizację we właściwości `.tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac05ad71ee90b1c800030849c5321cb7",
     "grade": true,
     "grade_id": "cell-f6cfe39258fbec51",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5582034830430798"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Prepare training data and sentiment label assigned to tweets\n",
    "tweets = [Counter(tweet.tokens) for tweet in training_set.tweets]\n",
    "y_train_tweets = [tweet.clazz for tweet in training_set.tweets]\n",
    "\n",
    "def classify_and_score(n_features = 16, x = tweets, y = y_train_tweets):\n",
    "    # Prepare separation in data - training and test data\n",
    "    index = int(len(x) * 0.8)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('feature_hasher', FeatureHasher(n_features)),\n",
    "        ('classifier', SVC())\n",
    "    ])\n",
    "    pipeline.fit(x[0:index], y[0:index])\n",
    "\n",
    "    # Score classifier\n",
    "    predicted = pipeline.predict(x[index+1:])\n",
    "    return accuracy_score(predicted, y[index+1:])\n",
    "\n",
    "classify_and_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd6bcaf8dae7184b60bd9a8adadd85d8",
     "grade": false,
     "grade_id": "cell-1caf16c401c91ef2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Stwórz wykres zależności wybranej miary klasyfikacji od wymiarów macierzy danych (chodzi o liczbę cech do których haszujemy cechy oryginalne). Wystarczy przetestować kilka (>=4) wybranych wartości na skali logarytmicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd253bac561b269cff3a3dceadc70f0",
     "grade": true,
     "grade_id": "cell-8076c16242981ae9",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores result and n-features size\n",
    "scores = list()\n",
    "n_features_list = 2**np.arange(2, 13)\n",
    "\n",
    "# Calculate scores\n",
    "for n_features in n_features_list:\n",
    "    scores.append(classify_and_score(n_features = n_features))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Liczba cech')\n",
    "plt.ylabel('Wynik klasyfikacji')\n",
    "plt.title('Miara klasyfikacji od liczby cech')\n",
    "plt.plot(n_features_list, scores, linestyle='', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82f3f52a6fe2a10a300b5d45101b32b5",
     "grade": false,
     "grade_id": "cell-eab7c2a5f0251ff4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - Obserwując stworzony wykres - skomentuj. Jak dużo jakości klasyfikacji się traci (albo zyskuje?) korzystając z mniejszej liczby haszowanych cech? Często klasyfikatory bardzo dobrze działają nawet przy liczbie haszowanych cech dla których na pewno istnieją konflikty cech oryginalnych - jak myślisz dlaczego? (Pomyśl o interpretacji takich skonfliktowanych cech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed30f2d487da41cf1a92ffb63195d621",
     "grade": true,
     "grade_id": "cell-2caea1821af5d8aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Zaobserwowano co następuje:\n",
    "* Już kilka cech pozwala na dobre odzwierciedlenie charakteru danych tj. pozwolić klasyfikatorowi na poprawne działanie - mogą mieć wtedy znaczenie najbardziej istotne atrybuty\n",
    "* Rozszerzanie cech może wpłynąć negatywnie na rezultat (pogorszyć jakość klasyfikacji) - klasyfikator może mieć problemy z wybraniem najbardziej istotnych cech\n",
    "* Liczba cech powinna pozwalać na przedstawienie jak największej liczby niezbędnych atrybutów, a jednocześnie nie być zbyt duża - zwiększanie liczby cech może wpłynąć nieznacznie na jakość rozwiązania jednocześnie wydłużając znacząco przetwarzanie\n",
    "* Skonfliktowane cechy mogą poprawnie należeć \"do jednej grupy\" tj. promować istotność danego atrybutu, co może być kwintesencją działania algorytmu w przypadku małej liczby cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20139da166319b49eea5cc7e984fc08e",
     "grade": false,
     "grade_id": "cell-0d86672dbabbf54d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - W poprzednim zadaniu wczytałeś wynik grupowania Browna do pamięci. Wytrenuj klasyfikator na reprezentacji ,,Bag-of-clusters'' tj. w kolumnach zamiast słów/n-gramów będziesz miał grupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b13c0457af5dab17e12780eafb1c5ac4",
     "grade": true,
     "grade_id": "cell-55264f6fe514d007",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5802016498625114"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Prepare translations word to cluster code\n",
    "word2cluster = { row.word: str(row.cluster) for row in brown_df.itertuples() }\n",
    "\n",
    "# Translate words in tweets into clusters\n",
    "clusters = [[word2cluster.get(token, \"-1\") for token in tweet.tokens] for tweet in training_set.tweets]\n",
    "\n",
    "# Prepare Bag of cluster representation\n",
    "data = list()\n",
    "for codes in clusters:\n",
    "    dictionary = {}\n",
    "    for code in codes:\n",
    "        dictionary[code] = dictionary.get(code, 0) + 1\n",
    "    data.append(dictionary)\n",
    "\n",
    "# Separate data - prepare test and train set\n",
    "index = int(len(clusters) * 0.8)\n",
    "\n",
    "# Prepare operation pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Fit classifier\n",
    "pipeline.fit(data[0:index], y_train_tweets[0:index])\n",
    "\n",
    "# Predict classes\n",
    "predicted = pipeline.predict(data[index+1:])\n",
    "\n",
    "# Check accuracy\n",
    "accuracy_score(predicted, y_train_tweets[index+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e47a053ebc12ac2fd97d9c11187da9b",
     "grade": false,
     "grade_id": "cell-493071698fc0205e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Podsumuj eksperymenty: poznałeś dwie możliwości ograniczenia liczby cech - zastąpienie słów ich grupami i haszowanie cech. Jakie są wady i zalety obydwu podejść?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80ace505afba9b12fd5d3896a9046ef",
     "grade": true,
     "grade_id": "cell-4508400659f7243e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
